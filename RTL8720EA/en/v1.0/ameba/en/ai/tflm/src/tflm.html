<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; RTL8720EA Docs  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/tabs.css?v=a5c4661c" />

  
    <link rel="shortcut icon" href="../../../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../../../_static/toggleprompt.js?v=d7ede5d2"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../../../../../_static/togglebutton.js?v=4a39c7ea"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
    <link rel="next" title="AIVoice" href="../../../ai_voice/src/index.html" />
    <link rel="prev" title="TensorFlow Lite for Microcontrollers (TFLM)" href="index.html" />  
     
  <link href="https://cdn.jsdelivr.net/npm/sweetalert2@11/dist/sweetalert2.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11"></script>
  <script type="text/javascript" src="../../../../../_static/js/selector.js"></script>
<style>
      .wy-nav-content {
      max-width: 1000px; /* 设置最大宽度 */
      }
</style>



</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            RTL8720EA Docs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../product_overview/src/index.html">Product Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../application_note/index.html">Application Note</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fullmac/src/index.html">FullMAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../wifi/index.html">Wi-Fi Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../wifi_tunnel/src/index.html">Wi-Fi R-Mesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../wifi_bridge/src/index.html">Wi-Fi Bridge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../wifi_csi/src/index.html">Wi-Fi CSI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dsp/index.html">DSP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../usb/usb_otg/src/index.html">USB Host &amp; Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../audio/src/index.html">Audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../multimedia/src/index.html">Multimedia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../gui/src/index.html">GUI</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">AI</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="index.html">TensorFlow Lite for Microcontrollers (TFLM)</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#supported-realtek-ameba-socs">Supported Realtek Ameba SoCs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-tensorflow-lite-micro-library">Build Tensorflow Lite Micro Library</a><ul>
<li><a class="reference internal" href="#km4">KM4</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-examples">Build Examples</a><ul>
<li><a class="reference internal" href="#id4">KM4</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mnist">MNIST</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ai_voice/src/index.html">AIVoice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../low_power/src/index.html">Low Power Solution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security_encryption/src/index.html">Security &amp; Encryption</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../at_command/src/index.html">AT Command</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../software_tools/index.html">Software Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mp_tools/src/index.html">MP Tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">RTL8720EA Docs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Artificial Intelligence (AI)</a></li>
          <li class="breadcrumb-item"><a href="index.html">TensorFlow Lite for Microcontrollers (TFLM)</a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction">
<span id="tflm"></span><h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">TensorFlow Lite for Microcontrollers</a> is an open-source library, it is a port of TensorFlow Lite designed to run machine learning models on DSPs, microcontrollers and other devices with limited memory.</p>
<p>Ameba-tflite-micro is a version of the TensorFlow Lite Micro library for Realtek Ameba SoCs with platform specific optimizations, and is available in <a class="reference external" href="https://github.com/Ameba-AIoT/ameba-rtos">ameba-rtos</a>.</p>
<p>Links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">tflite-micro github repository</a></p></li>
<li><p><a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/overview">tflite-micro document</a></p></li>
</ul>
<section id="supported-realtek-ameba-socs">
<h2>Supported Realtek Ameba SoCs<a class="headerlink" href="#supported-realtek-ameba-socs" title="Link to this heading"></a></h2>
<table class="docutils align-default" style="width: 100%">
<thead>
<tr class="row-odd"><th class="head"><p>OS</p></th>
<th class="head"><p>Chip</p></th>
<th class="head"><p>Processor</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p>FreeRTOS</p></td>
<td><p>RTL8730E</p></td>
<td><p>CA32</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8713EC,RTL8726EA</p></td>
<td><p>HiFi5 DSP</p></td>
</tr>
<tr class="row-even"><td><p>RTL8710EC,RTL8713EC,RTL8720EA,RTL8726EA</p></td>
<td><p>KM4</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8721Dx,RTL8711Dx</p></td>
<td><p>KM4</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="build-tensorflow-lite-micro-library">
<span id="build-tflm-lib"></span><h1>Build Tensorflow Lite Micro Library<a class="headerlink" href="#build-tensorflow-lite-micro-library" title="Link to this heading"></a></h1>
<section id="km4">
<h2>KM4<a class="headerlink" href="#km4" title="Link to this heading"></a></h2>
<p>To build Tensorflow Lite Micro Library for KM4, enable tflite_micro configuration in SDK menuconfig.</p>
<ol class="arabic">
<li><p>Switch to gcc project directory</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="p">{</span><span class="n">SDK</span><span class="p">}</span><span class="o">/</span><span class="n">amebalite_gcc_project</span>
<span class="o">./</span><span class="n">menuconfig</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</li>
<li><p>Choose Kernel KM4</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../../../../_images/menuconfig_lite_km4.png"><img alt="../../../../../_images/menuconfig_lite_km4.png" src="../../../../../_images/menuconfig_lite_km4.png" style="width: 471.0px; height: 416.0px;" /></a>
</figure>
</li>
<li><p>Choose AI config</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../../../../_images/menuconfig_lite_km4_ai.png"><img alt="../../../../../_images/menuconfig_lite_km4_ai.png" src="../../../../../_images/menuconfig_lite_km4_ai.png" style="width: 512.0px; height: 187.0px;" /></a>
</figure>
</li>
<li><p>Enable tflite_micro</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../../../../_images/menuconfig_lite_km4_ai_tflm.png"><img alt="../../../../../_images/menuconfig_lite_km4_ai_tflm.png" src="../../../../../_images/menuconfig_lite_km4_ai_tflm.png" style="width: 406.0px; height: 88.0px;" /></a>
</figure>
</li>
</ol>
</section>
</section>
<section id="build-examples">
<span id="build-tflm-example"></span><h1>Build Examples<a class="headerlink" href="#build-examples" title="Link to this heading"></a></h1>
<section id="id4">
<h2>KM4<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
</section>
<p>TensorFlow Lite for Microcontrollers related examples are in the <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro</span></code> directory. To build an example image such as tflm_hello_world:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">build</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">a</span> <span class="n">tflm_hello_world</span>
</pre></div>
</div>
</section>
<section id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Link to this heading"></a></h1>
<section id="mnist">
<h2>MNIST<a class="headerlink" href="#mnist" title="Link to this heading"></a></h2>
<section id="id5">
<h3>Introduction<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<p>The <em>MNIST</em> database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. In this tutorial, MNIST database is used to show a full workflow <strong>from training a model to deploying it and run inference</strong> on Ameba SoCs with tflite-micro.</p>
<p>Example codes are in the <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro/tflm_mnist</span></code> directory.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Step 1-4 are for preparing necessary files on a development machine (server or PC etc.). You can skip them and use prepared files to build the image.</p>
</div>
</section>
<section id="step-1-train-a-model">
<h3>Step 1. Train a Model<a class="headerlink" href="#step-1-train-a-model" title="Link to this heading"></a></h3>
<p>First train and evaluate a classification model for 10 digits of MNIST dataset. You can choose either <strong>keras(tensorflow)</strong> or <strong>pytorch</strong> framework by running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">keras_train_eval.py</span> <span class="pre">--output</span> <span class="pre">keras_mnist_conv</span></code> or <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">torch_train_eval.py</span> <span class="pre">--output</span> <span class="pre">torch_mnist_conv</span></code>.</p>
<p>A simple convolution based model will be trained for several epochs and then accuracy will be tested.</p>
<p>Due to the limited <strong>computation resources</strong> and <strong>memory</strong> of microcontrollers, we recommend paying attention to <strong>model size</strong> and <strong>operation numbers</strong>.</p>
<ul>
<li><p>Use <em>keras_flops</em> library under tensorflow/keras framework:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras_flops</span> <span class="kn">import</span> <span class="n">get_flops</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">flops</span> <span class="o">=</span> <span class="n">get_flops</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use <em>ptflops</em> library under pytorch framework:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ptflops</span> <span class="kn">import</span> <span class="n">get_model_complexity_info</span>

<span class="n">macs</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">get_model_complexity_info</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>After training, keras model is saved in SavedModel format. Pytorch model is saved in .pt format, while a .onnx file is also exported for later conversion stage.</p>
</section>
<section id="step-2-convert-to-tflite">
<h3>Step 2. Convert to Tflite<a class="headerlink" href="#step-2-convert-to-tflite" title="Link to this heading"></a></h3>
<p>In this stage, <strong>post-training integer quantization</strong> is applied on the trained model and output .tflite format. Float model inference is also supported on Ameba SoCs, however, we recommend using integer quantization which can extremely reduce computation and memory with little performance degradation.</p>
<ul>
<li><p>For models trained by keras(tensorflow), run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span><span class="o">/</span><span class="n">saved_model</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span>
</pre></div>
</div>
</li>
<li><p>For models trained by pytorch, run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span>
</pre></div>
</div>
</li>
</ul>
<p>An additional step will run to convert from .onnx to SavedModel format.</p>
<p>Then <em>tf.lite.TFLiteConverter</em> is used to convert SavedModel into int8 .tflite given a representative dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">Optimize</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">representative_dataset</span> <span class="o">=</span> <span class="n">repr_dataset</span>
<span class="n">converter</span><span class="o">.</span><span class="n">target_spec</span><span class="o">.</span><span class="n">supported_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">OpsSet</span><span class="o">.</span><span class="n">TFLITE_BUILTINS_INT8</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_input_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_output_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">tflite_int8_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>
</pre></div>
</div>
<p>Refer to <a class="reference external" href="https://ai.google.dev/edge/litert/models/post_training_integer_quant#convert_using_integer-only_quantization">tflite official site</a> for more details about integer-only quantization.</p>
<p>After conversion, the performance on test set will be validated using int8 .tflite model and two .npy files containing input array and label array of 100 test images are generated for later use on SoC.</p>
<p>In <code class="file docutils literal notranslate"><span class="pre">convert.py</span></code>, <a class="reference external" href="https://github.com/onnx/onnx-tensorflow">onnx_tf library</a> is used for converting from onnx to SavedModel. Other convert libraries are available with similar purpose:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PINTO0309/onnx2tf">onnx2tf</a></p></li>
<li><p><a class="reference external" href="https://github.com/google-ai-edge/ai-edge-torch">ai-edge-torch</a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexanderLutsenko/nobuco">nobuco</a></p></li>
<li><p><a class="reference external" href="https://github.com/MPolaris/onnx2tflite">onnx2tflite</a></p></li>
</ul>
</section>
<section id="step-3-optimize-tflite-and-convert-to-c">
<h3>Step 3. Optimize Tflite and Convert to C++<a class="headerlink" href="#step-3-optimize-tflite-and-convert-to-c" title="Link to this heading"></a></h3>
<ol class="arabic">
<li><p>Use <strong>tflm_model_transforms</strong> tool from official tflite-micro repository can reduce .tflite size by running some TFLM specific transformations. It also re-align the tflite flatbuffer via the C++ flatbuffer api which can speed up inference on some Ameba platforms. This step is optional, but we strongly recommend running it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">tflite</span><span class="o">-</span><span class="n">micro</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">tflite</span><span class="o">-</span><span class="n">micro</span>

<span class="n">bazel</span> <span class="n">build</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="p">:</span><span class="n">tflm_model_transforms</span>
<span class="n">bazel</span><span class="o">-</span><span class="nb">bin</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">tflm_model_transforms</span> <span class="o">--</span><span class="n">input_model_path</span><span class="o">=&lt;/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">my_model</span><span class="o">.</span><span class="n">tflite</span><span class="o">&gt;</span>

<span class="c1"># output will be located at: /path/to/my_model_tflm_optimized.tflite</span>
</pre></div>
</div>
</li>
<li><p>Convert .tflite model and .npy test data to .cc and .h files for deployment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">models</span> <span class="n">int8_tflm_optimized</span><span class="o">.</span><span class="n">tflite</span>
<span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">testdata</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="step-4-inference-on-soc-with-tflite-micro">
<h3>Step 4. Inference on SoC with Tflite-Micro<a class="headerlink" href="#step-4-inference-on-soc-with-tflite-micro" title="Link to this heading"></a></h3>
<p><code class="file docutils literal notranslate"><span class="pre">example_tflm_mnist.cc</span></code> shows how to run inference with the trained model on test data, calculate accuracy, profile memory and latency.</p>
<p>Use <a class="reference external" href="https://netron.app/">netron</a> to visualize the .tflite file and check the operations used by the model. Instantiate operations resolver to register and access the operations.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">MnistOpResolver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tflite</span><span class="o">::</span><span class="n">MicroMutableOpResolver</span><span class="o">&lt;</span><span class="mi">4</span><span class="o">&gt;</span><span class="p">;</span>

<span class="n">TfLiteStatus</span><span class="w"> </span><span class="nf">RegisterOps</span><span class="p">(</span><span class="n">MnistOpResolver</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op_resolver</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddFullyConnected</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddConv2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddMaxPool2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddReshape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kTfLiteOk</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Refer to <a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/get_started#run_inference">tflite-micro official site</a> for more details about running inference with tflite-micro.</p>
</section>
<section id="step-5-build-example">
<h3>Step 5. Build Example<a class="headerlink" href="#step-5-build-example" title="Link to this heading"></a></h3>
<p>Follow steps in <a class="reference internal" href="#build-tflm-lib"><span class="std std-ref">Build Tensorflow Lite Micro Library</span></a> and <a class="reference internal" href="#build-tflm-example"><span class="std std-ref">Build Examples</span></a> to build the example image.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="TensorFlow Lite for Microcontrollers (TFLM)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../../ai_voice/src/index.html" class="btn btn-neutral float-right" title="AIVoice" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Realsil.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>