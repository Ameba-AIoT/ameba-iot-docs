

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TensorFlow Lite for Microcontrollers (TFLM) &mdash; Ameba IoT Docs  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=b0e775ca" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=a5c4661c" />
      <link rel="stylesheet" type="text/css" href="../../_static/_static/custom.css" />

  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/toggleprompt.js?v=d7ede5d2"></script>
        <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="AIVoice" href="../4_ai_voice/0_ai_voice_index.html" />
    <link rel="prev" title="Artificial Intelligence (AI)" href="0_ai_index.html" />
   
  
  <script type="text/javascript" src="../../_static/js/selector.js"></script>

  <style>
    .wy-nav-content {max-width: 1000px;} /* 设置最大宽度 */
  </style>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Ameba IoT Docs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_product/0_ameba_product_center_index.html">Products Center</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_sdk/0_ameba_sdks_index.html">Ameba SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_solutions/0_ameba_solutions_index.html">Ameba Solutions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gcc_build_environment/0_gcc_build_index.html">0.1 GCC Build Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gdb_debug/0_gdb_debug_index.html">0.2 GDB Debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gcc_build_library/0_gcc_build_library_index.html">0.3 GCC Build Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_sdk_example/0_sdk_example_index.html">0.4 SDK Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_memory_layout/0_layout_index.html">0.5 Flash &amp; RAM Layout</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_user_config/0_usrcfg_index.html">0.6 User Config</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_tools/0_tools_index.html">0.7 Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_at_command/0_at_command_index.html">0.8 AT Command</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_file_system/0_vfs_index.html">0.9 Virtual File System</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_mpu_cache/0_mpu_cache_index.html">1.0 MPU and Cache</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_boot_process/0_boot_index.html">1.1 BOOT Process</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_ota/0_ota_index.html">1.2 OTA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_otpc/0_otpc_index.html">1.3 OTPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_chipen/0_chipen_index.html">1.4 CHIPEN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_ipc/0_ipc_index.html">1.5 Inter Processor Communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_pinmux/0_pinmux_index.html">1.6 PINMUX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_gpio/0_gpio_index.html">1.7 GPIO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_power_save/0_ps_index.html">1.9 Power Save</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_whc_wifi_bridge/0_wifi_bridge_index.html">2.1 WHC Bridge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_whc_fullmac/0_fullmac_index.html">2.2 WHC FullMAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_tunnel/0_wifi_tunnel_index.html">2.3 Wi-Fi R-Mesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_csi/0_wifi_csi_index.html">2.5 Wi-Fi CSI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_adaptivity_test/0_wifi_adaptivity_index.html">2.6 Wi-Fi Adaptivity Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_security/0_security_index.html">3.1 Security &amp; Encryption</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_trng/0_trng_index.html">3.2 True Random Number Generator</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_huk_derivation/0_huk_derivation_index_nda.html">3.3 HUK Derivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_rdp/0_rdp_index_nda.html">3.4 RDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_rsip/0_rsip_index_nda.html">3.5 RSIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_secure_boot/0_secure_boot_index_nda.html">3.6 Secure Boot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_swd_protection/0_swd_protection_index_nda.html">3.7 SWD Protection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_tfm/0_tfm_index_nda.html">3.8 Trusted Firmware-M (TF-M)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_crypto_engine/0_crypto_engine_index.html">3.9 Crypto Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ecdsa_engine/0_ecdsa_index.html">3.a ECDSA Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_eddsa_engine/0_eddsa_index.html">3.b EDDSA Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_rsa_engine/0_rsa_index.html">3.c RSA Engine</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="0_ai_index.html">4.1 AI</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">TensorFlow Lite for Microcontrollers (TFLM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#supported-realtek-ameba-socs">Supported Realtek Ameba SoCs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-tensorflow-lite-micro-library">Build Tensorflow Lite Micro Library</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-examples">Build Examples</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mnist">MNIST</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../4_ai_voice/0_ai_voice_index.html">4.2 AIVoice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_multimedia/0_multimedia_index.html">4.3 Multimedia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dsp/0_dsp_index.html">4.4 DSP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mass_production/0_mp_index.html">6.1 Mass Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mp_image/0_mp_image_index.html">6.2 MP Image</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mptools/0_mptools_index.html">6.3 MP Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../7_usb/0_usb_index.html">7.1 USB Host &amp; Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_dmac/0_dmac_index.html">8.1 DMA Controllor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_psram/0_psram_index.html">8.2 PSRAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_thermal/0_thermal_index.html">8.3 Thermal Meter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_adc/0_adc_index.html">8.4 ADC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_ir/0_ir_index.html">8.5 IR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_ledc/0_ledc_index.html">8.6 LEDC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_cap_touch/0_cap_touch_index.html">8.7 Cap-Touch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_key_scan/0_key_scan_index.html">8.8 Key-Scan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_rtc_io/0_rtc_io_index.html">8.9 RTC-IO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_lcdc/0_lcdc_index.html">8.A LCDC</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Ameba IoT Docs</a>
      </nav>

      <div class="wy-nav-content">

        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="0_ai_index.html">Artificial Intelligence (AI)</a></li>
      <li class="breadcrumb-item active">TensorFlow Lite for Microcontrollers (TFLM)</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tensorflow-lite-for-microcontrollers-tflm">
<h1>TensorFlow Lite for Microcontrollers (TFLM)<a class="headerlink" href="#tensorflow-lite-for-microcontrollers-tflm" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">TensorFlow Lite for Microcontrollers</a> is an open-source library, it is a port of TensorFlow Lite designed to run machine learning models on DSPs, microcontrollers and other devices with limited memory.</p>
<p>Ameba-tflite-micro is a version of the TensorFlow Lite Micro library for Realtek Ameba SoCs with platform specific optimizations, and is available in <a class="reference external" href="https://github.com/Ameba-AIoT/ameba-rtos">ameba-rtos</a>.</p>
<p>Links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">tflite-micro github repository</a></p></li>
<li><p><a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/overview">tflite-micro document</a></p></li>
</ul>
<section id="supported-realtek-ameba-socs">
<h3>Supported Realtek Ameba SoCs<a class="headerlink" href="#supported-realtek-ameba-socs" title="Link to this heading"></a></h3>
<table class="docutils align-default" style="width: 100%">
<thead>
<tr class="row-odd"><th class="head"><p>OS</p></th>
<th class="head"><p>Chip</p></th>
<th class="head"><p>Processor</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p>FreeRTOS</p></td>
<td><p>RTL8730E</p></td>
<td><p>CA32</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8713EC,RTL8726EA</p></td>
<td><p>HiFi5 DSP</p></td>
</tr>
<tr class="row-even"><td><p>RTL8710EC,RTL8713EC,RTL8720EA,RTL8726EA</p></td>
<td><p>KM4</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8721Dx,RTL8711Dx</p></td>
<td><p>KM4</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="build-tensorflow-lite-micro-library">
<span id="build-tflm-lib"></span><h2>Build Tensorflow Lite Micro Library<a class="headerlink" href="#build-tensorflow-lite-micro-library" title="Link to this heading"></a></h2>
</section>
<section id="build-examples">
<span id="build-tflm-example"></span><h2>Build Examples<a class="headerlink" href="#build-examples" title="Link to this heading"></a></h2>
<p>TensorFlow Lite for Microcontrollers related examples are in the <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro</span></code> directory. To build an example image such as tflm_hello_world:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">build</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">a</span> <span class="n">tflm_hello_world</span>
</pre></div>
</div>
</section>
<section id="tutorial">
<h2>Tutorial<a class="headerlink" href="#tutorial" title="Link to this heading"></a></h2>
<section id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Link to this heading"></a></h3>
<section id="id5">
<h4>Introduction<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<p>The <em>MNIST</em> database (Modified National Institute of Standards and Technology database) is a large collection of handwritten digits. In this tutorial, MNIST database is used to show a full workflow <strong>from training a model to deploying it and run inference</strong> on Ameba SoCs with tflite-micro.</p>
<p>Example codes are in the <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro/tflm_mnist</span></code> directory.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Step 1-4 are for preparing necessary files on a development machine (server or PC etc.). You can skip them and use prepared files to build the image.</p>
</div>
</section>
<section id="step-1-train-a-model">
<h4>Step 1. Train a Model<a class="headerlink" href="#step-1-train-a-model" title="Link to this heading"></a></h4>
<p>First train and evaluate a classification model for 10 digits of MNIST dataset. You can choose either <strong>keras(tensorflow)</strong> or <strong>pytorch</strong> framework by running <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">keras_train_eval.py</span> <span class="pre">--output</span> <span class="pre">keras_mnist_conv</span></code> or <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">torch_train_eval.py</span> <span class="pre">--output</span> <span class="pre">torch_mnist_conv</span></code>.</p>
<p>A simple convolution based model will be trained for several epochs and then accuracy will be tested.</p>
<p>Due to the limited <strong>computation resources</strong> and <strong>memory</strong> of microcontrollers, we recommend paying attention to <strong>model size</strong> and <strong>operation numbers</strong>.</p>
<ul>
<li><p>Use <em>keras_flops</em> library under tensorflow/keras framework:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras_flops</span> <span class="kn">import</span> <span class="n">get_flops</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">flops</span> <span class="o">=</span> <span class="n">get_flops</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use <em>ptflops</em> library under pytorch framework:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ptflops</span> <span class="kn">import</span> <span class="n">get_model_complexity_info</span>

<span class="n">macs</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">get_model_complexity_info</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>After training, keras model is saved in SavedModel format. Pytorch model is saved in .pt format, while a .onnx file is also exported for later conversion stage.</p>
</section>
<section id="step-2-convert-to-tflite">
<h4>Step 2. Convert to Tflite<a class="headerlink" href="#step-2-convert-to-tflite" title="Link to this heading"></a></h4>
<p>In this stage, <strong>post-training integer quantization</strong> is applied on the trained model and output .tflite format. Float model inference is also supported on Ameba SoCs, however, we recommend using integer quantization which can extremely reduce computation and memory with little performance degradation.</p>
<ul>
<li><p>For models trained by keras(tensorflow), run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span><span class="o">/</span><span class="n">saved_model</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span>
</pre></div>
</div>
</li>
<li><p>For models trained by pytorch, run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span>
</pre></div>
</div>
</li>
</ul>
<p>An additional step will run to convert from .onnx to SavedModel format.</p>
<p>Then <em>tf.lite.TFLiteConverter</em> is used to convert SavedModel into int8 .tflite given a representative dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">Optimize</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">representative_dataset</span> <span class="o">=</span> <span class="n">repr_dataset</span>
<span class="n">converter</span><span class="o">.</span><span class="n">target_spec</span><span class="o">.</span><span class="n">supported_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">OpsSet</span><span class="o">.</span><span class="n">TFLITE_BUILTINS_INT8</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_input_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_output_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">tflite_int8_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>
</pre></div>
</div>
<p>Refer to <a class="reference external" href="https://ai.google.dev/edge/litert/models/post_training_integer_quant#convert_using_integer-only_quantization">tflite official site</a> for more details about integer-only quantization.</p>
<p>After conversion, the performance on test set will be validated using int8 .tflite model and two .npy files containing input array and label array of 100 test images are generated for later use on SoC.</p>
<p>In <code class="file docutils literal notranslate"><span class="pre">convert.py</span></code>, <a class="reference external" href="https://github.com/onnx/onnx-tensorflow">onnx_tf library</a> is used for converting from onnx to SavedModel. Other convert libraries are available with similar purpose:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PINTO0309/onnx2tf">onnx2tf</a></p></li>
<li><p><a class="reference external" href="https://github.com/google-ai-edge/ai-edge-torch">ai-edge-torch</a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexanderLutsenko/nobuco">nobuco</a></p></li>
<li><p><a class="reference external" href="https://github.com/MPolaris/onnx2tflite">onnx2tflite</a></p></li>
</ul>
</section>
<section id="step-3-optimize-tflite-and-convert-to-c">
<h4>Step 3. Optimize Tflite and Convert to C++<a class="headerlink" href="#step-3-optimize-tflite-and-convert-to-c" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Use <strong>tflm_model_transforms</strong> tool from official tflite-micro repository can reduce .tflite size by running some TFLM specific transformations. It also re-align the tflite flatbuffer via the C++ flatbuffer api which can speed up inference on some Ameba platforms. This step is optional, but we strongly recommend running it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">tflite</span><span class="o">-</span><span class="n">micro</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">tflite</span><span class="o">-</span><span class="n">micro</span>

<span class="n">bazel</span> <span class="n">build</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="p">:</span><span class="n">tflm_model_transforms</span>
<span class="n">bazel</span><span class="o">-</span><span class="nb">bin</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">tflm_model_transforms</span> <span class="o">--</span><span class="n">input_model_path</span><span class="o">=&lt;/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">my_model</span><span class="o">.</span><span class="n">tflite</span><span class="o">&gt;</span>

<span class="c1"># output will be located at: /path/to/my_model_tflm_optimized.tflite</span>
</pre></div>
</div>
</li>
<li><p>Convert .tflite model and .npy test data to .cc and .h files for deployment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">models</span> <span class="n">int8_tflm_optimized</span><span class="o">.</span><span class="n">tflite</span>
<span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">testdata</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="step-4-inference-on-soc-with-tflite-micro">
<h4>Step 4. Inference on SoC with Tflite-Micro<a class="headerlink" href="#step-4-inference-on-soc-with-tflite-micro" title="Link to this heading"></a></h4>
<p><code class="file docutils literal notranslate"><span class="pre">example_tflm_mnist.cc</span></code> shows how to run inference with the trained model on test data, calculate accuracy, profile memory and latency.</p>
<p>Use <a class="reference external" href="https://netron.app/">netron</a> to visualize the .tflite file and check the operations used by the model. Instantiate operations resolver to register and access the operations.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">MnistOpResolver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tflite</span><span class="o">::</span><span class="n">MicroMutableOpResolver</span><span class="o">&lt;</span><span class="mi">4</span><span class="o">&gt;</span><span class="p">;</span>

<span class="n">TfLiteStatus</span><span class="w"> </span><span class="nf">RegisterOps</span><span class="p">(</span><span class="n">MnistOpResolver</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op_resolver</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddFullyConnected</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddConv2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddMaxPool2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddReshape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kTfLiteOk</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Refer to <a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/get_started#run_inference">tflite-micro official site</a> for more details about running inference with tflite-micro.</p>
</section>
<section id="step-5-build-example">
<h4>Step 5. Build Example<a class="headerlink" href="#step-5-build-example" title="Link to this heading"></a></h4>
<p>Follow steps in <a class="reference internal" href="#build-tflm-lib"><span class="std std-ref">Build Tensorflow Lite Micro Library</span></a> and <a class="reference internal" href="#build-tflm-example"><span class="std std-ref">Build Examples</span></a> to build the example image.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="0_ai_index.html" class="btn btn-neutral float-left" title="Artificial Intelligence (AI)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../4_ai_voice/0_ai_voice_index.html" class="btn btn-neutral float-right" title="AIVoice" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Realsil.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>

      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>