

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TensorFlow Lite for Microcontrollers (TFLM) &mdash; Ameba IoT Docs  文档</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom_cn.css?v=0d22fac7" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=a5c4661c" />
      <link rel="stylesheet" type="text/css" href="../../_static/_static/custom_cn.css" />

  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=7d86a446"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/toggleprompt.js?v=d7ede5d2"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
        <script src="../../_static/translations.js?v=beaddf03"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="AIVoice" href="../4_ai_voice/0_ai_voice_index_cn.html" />
    <link rel="prev" title="Artificial Intelligence (AI)" href="0_ai_index_cn.html" />
   
  
  <script type="text/javascript" src="../../_static/js/selector.js"></script>

  <style>
    .wy-nav-content {max-width: 1000px;} /* 设置最大宽度 */
  </style>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Ameba IoT Docs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_product/0_ameba_product_center_index_cn.html">产品中心</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_sdk/0_ameba_sdks_index_cn.html">Ameba SDK</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_ameba_solutions/0_ameba_solutions_index_cn.html">解决方案</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gcc_build_environment/0_gcc_build_index_cn.html">0.1 GCC 编译环境</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gdb_debug/0_gdb_debug_index_cn.html">0.2 GDB调试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_gcc_build_library/0_gcc_build_library_index_cn.html">0.3 GCC 库的生成和使用</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_sdk_example/0_sdk_example_index_cn.html">0.4 SDK 示例</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_memory_layout/0_layout_index_cn.html">0.5 Flash 和 RAM 布局</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_user_config/0_usrcfg_index_cn.html">0.6 开发者配置</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_tools/0_tools_index_cn.html">0.7 工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_at_command/0_at_command_index_cn.html">0.8 AT 命令</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_file_system/0_vfs_index_cn.html">0.9 虚拟文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_ftl/0_ftl_index_cn.html">0.A Flash 转换层</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_mpu_cache/0_mpu_cache_index_cn.html">1.0 内存保护单元和缓存</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_boot_process/0_boot_index_cn.html">1.1 启动过程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_ota/0_ota_index_cn.html">1.2 固件在线升级(OTA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_otpc/0_otpc_index.html">1.3 OTP存储器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_chipen/0_chipen_index_cn.html">1.4 CHIP_EN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_ipc/0_ipc_index_cn.html">1.5 核间通信</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_pinmux/0_pinmux_index_cn.html">1.6 PINMUX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_gpio/0_gpio_index_cn.html">1.7 GPIO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_power_save/0_ps_index_cn.html">1.9 低功耗开发</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_whc_wifi_bridge/0_wifi_bridge_index_cn.html">2.1 WHC Bridge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_whc_fullmac/0_fullmac_index_cn.html">2.2 WHC FullMAC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_tunnel/0_wifi_tunnel_index_cn.html">2.3 Wi-Fi R-Mesh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_csi/0_wifi_csi_index_cn.html">2.5 Wi-Fi CSI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_wifi_adaptivity_test/0_wifi_adaptivity_index.html">2.6 Wi-Fi Adaptivity 测试指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_security/0_security_index_cn.html">3.1 安全与加密</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_trng/0_trng_index_cn.html">3.2 真随机数发生器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_huk_derivation/0_huk_derivation_index_nda.html">3.3 HUK Derivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_rdp/0_rdp_index_nda.html">3.4 RDP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_rsip/0_rsip_index_nda.html">3.5 RSIP</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_secure_boot/0_secure_boot_index_nda.html">3.6 Secure Boot</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_swd_protection/0_swd_protection_index_nda.html">3.7 SWD Protection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_nda_tfm/0_tfm_index_nda.html">3.8 Trusted Firmware-M (TF-M)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_crypto_engine/0_crypto_engine_index_cn.html">3.9 对称硬件加密引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ecdsa_engine/0_ecdsa_index_cn.html">3.a ECDSA 硬件加密引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_eddsa_engine/0_eddsa_index_cn.html">3.b EDDSA 硬件加密引擎</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_rsa_engine/0_rsa_index_cn.html">3.c RSA 硬件加密引擎</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="0_ai_index_cn.html">4.1 人工智能</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">TensorFlow Lite for Microcontrollers (TFLM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">概述</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">支持平台</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tensorflow-lite-micro">编译Tensorflow Lite Micro库</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#build-tflm-example">编译示例</a><ul class="simple">
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id8">教程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mnist">MNIST</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../4_ai_voice/0_ai_voice_index_cn.html">4.2 语音识别</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_multimedia/0_multimedia_index_cn.html">4.3 多媒体</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dsp/0_dsp_index_cn.html">4.4 DSP 使用指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mass_production/0_mp_index_cn.html">6.1 量产指导</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mp_image/0_mp_image_index_cn.html">6.2 MP 固件</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_mptools/0_mptools_index_cn.html">6.3 产测工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../7_usb/0_usb_index_cn.html">7.1 USB 主机与从机</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_dmac/0_dmac_index_cn.html">8.1 DMA 控制器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_psram/0_psram_index_cn.html">8.2 PSRAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_thermal/0_thermal_index_cn.html">8.3 温度测量</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_adc/0_adc_index_cn.html">8.4 模数转换器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_ir/0_ir_index_cn.html">8.5 红外收发器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_ledc/0_ledc_index_cn.html">8.6 LED控制器</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_cap_touch/0_cap_touch_index_cn.html">8.7 Cap-Touch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_key_scan/0_key_scan_index_cn.html">8.8 Key-Scan</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_rtc_io/0_rtc_io_index_cn.html">8.9 RTC-IO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../8_lcdc/0_lcdc_index_cn.html">8.A LCD控制器</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Ameba IoT Docs</a>
      </nav>

      <div class="wy-nav-content">

        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="0_ai_index_cn.html">Artificial Intelligence (AI)</a></li>
      <li class="breadcrumb-item active">TensorFlow Lite for Microcontrollers (TFLM)</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tensorflow-lite-for-microcontrollers-tflm">
<h1>TensorFlow Lite for Microcontrollers (TFLM)<a class="headerlink" href="#tensorflow-lite-for-microcontrollers-tflm" title="Link to this heading"></a></h1>
<section id="id1">
<h2>概述<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">TensorFlow Lite for Microcontrollers</a> 是一个开源库，它是TensorFlow Lite的一个移植版本，旨在在数字信号处理器（DSP）、微控制器和其他内存有限的设备上运行机器学习模型。</p>
<p>Ameba-tflite-micro是针对瑞昱Ameba芯片的TensorFlow Lite Micro库的一个版本，包含平台相关的优化，可以在 <a class="reference external" href="https://github.com/Ameba-AIoT/ameba-rtos">ameba-rtos</a> 中获取。</p>
<p>链接:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/tensorflow/tflite-micro">tflite-micro github 仓库</a></p></li>
<li><p><a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/overview">tflite-micro 文档</a></p></li>
</ul>
<section id="id2">
<h3>支持平台<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<table class="docutils align-default" style="width: 100%">
<thead>
<tr class="row-odd"><th class="head"><p>操作系统</p></th>
<th class="head"><p>芯片</p></th>
<th class="head"><p>核</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="4"><p>FreeRTOS</p></td>
<td><p>RTL8730E</p></td>
<td><p>CA32</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8713EC,RTL8726EA</p></td>
<td><p>HiFi5 DSP</p></td>
</tr>
<tr class="row-even"><td><p>RTL8710EC,RTL8713EC,RTL8720EA,RTL8726EA</p></td>
<td><p>KM4</p></td>
</tr>
<tr class="row-odd"><td><p>RTL8721Dx,RTL8711Dx</p></td>
<td><p>KM4</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="tensorflow-lite-micro">
<span id="build-tflm-lib"></span><h2>编译Tensorflow Lite Micro库<a class="headerlink" href="#tensorflow-lite-micro" title="Link to this heading"></a></h2>
</section>
<section id="build-tflm-example">
<span id="id4"></span><h2>编译示例<a class="headerlink" href="#build-tflm-example" title="Link to this heading"></a></h2>
<p>TensorFlow Lite for Microcontrollers相关的示例在  <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro</span></code> 目录下。编译固件，例如tflm_hello_world，执行：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">build</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">a</span> <span class="n">tflm_hello_world</span>
</pre></div>
</div>
</section>
<section id="id8">
<h2>教程<a class="headerlink" href="#id8" title="Link to this heading"></a></h2>
<section id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Link to this heading"></a></h3>
<section id="id9">
<h4>概述<a class="headerlink" href="#id9" title="Link to this heading"></a></h4>
<p><em>MNIST</em> (Modified National Institute of Standards and Technology database) 是一个大型手写体数字的图片数据集。本教程基于MNIST数据集，展示 <strong>从训练模型到部署</strong>，并在Ameba SoC上使用tflite-micro进行 <strong>推理</strong> 的一个完整流程。</p>
<p>示例代码在 <code class="docutils literal notranslate"><span class="pre">{SDK}/component/example/tflite_micro/tflm_mnist</span></code> 目录下。</p>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>步骤 1-4 是为了在开发机器（服务器或个人电脑等）上准备必要的文件。您也可以跳过这些步骤，直接使用准备好的文件来编译固件。</p>
</div>
</section>
<section id="id10">
<h4>步骤 1. 训练模型<a class="headerlink" href="#id10" title="Link to this heading"></a></h4>
<p>首先训练并评估基于MNIST数据集的10个数字的分类模型。您可以通过运行 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">keras_train_eval.py</span> <span class="pre">--output</span> <span class="pre">keras_mnist_conv</span></code> 或者 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">torch_train_eval.py</span> <span class="pre">--output</span> <span class="pre">torch_mnist_conv</span></code> 来选择 <strong>keras(tensorflow)</strong> 或者 <strong>pytorch</strong> 训练框架。</p>
<p>示例中所用模型是一个以卷积为主的简单网络结构，在数据集上进行几轮训练迭代后，再测试它的准确率。</p>
<p>由于微处理器上 <strong>算力</strong> 和 <strong>内存</strong> 都比较有限，我们建议关注 <strong>模型大小</strong> 以及 <strong>运算操作数</strong>。</p>
<ul>
<li><p>Tensorflow/Keras框架下可以使用 <em>keras_flops</em> 库：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras_flops</span> <span class="kn">import</span> <span class="n">get_flops</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">flops</span> <span class="o">=</span> <span class="n">get_flops</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Pytorch框架下可以使用 <em>ptflops</em> 库：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ptflops</span> <span class="kn">import</span> <span class="n">get_model_complexity_info</span>

<span class="n">macs</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">get_model_complexity_info</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span> <span class="n">as_strings</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>训练完成后，keras模型以SavedModel格式保存。Pytorch模型则以.pt格式保存，同时还会导出一个.onnx文件，用于后续的转换。</p>
</section>
<section id="tflite">
<h4>步骤 2. 转换成Tflite文件<a class="headerlink" href="#tflite" title="Link to this heading"></a></h4>
<p>在这个步骤中，对训练好的模型应用 <strong>训练后整数量化</strong>，并输出为.tflite格式。Ameba SoC也支持浮点模型的推理，但我们仍建议使用整数量化，因为它可以在性能下降很小的情况下，极大地减少计算量和内存。</p>
<ul>
<li><p>对于使用keras(tensorflow)训练的模型，运行</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span><span class="o">/</span><span class="n">saved_model</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">keras_mnist_conv</span>
</pre></div>
</div>
</li>
<li><p>对于使用pytorch训练的模型，运行</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">convert</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="nb">input</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span><span class="o">/</span><span class="n">model</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">output</span><span class="o">-</span><span class="n">path</span> <span class="n">torch_mnist_conv</span>
</pre></div>
</div>
<p>有一个额外的步骤会将模型从.onnx格式转换成SavedModel格式。</p>
</li>
</ul>
<p>然后给定一个参考数据集，使用 <em>tf.lite.TFLiteConverter</em> 把SavedModel格式转换成int8的.tflite格式模型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_saved_model</span><span class="p">(</span><span class="n">saved_model_dir</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">Optimize</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">representative_dataset</span> <span class="o">=</span> <span class="n">repr_dataset</span>
<span class="n">converter</span><span class="o">.</span><span class="n">target_spec</span><span class="o">.</span><span class="n">supported_ops</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">OpsSet</span><span class="o">.</span><span class="n">TFLITE_BUILTINS_INT8</span><span class="p">]</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_input_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">converter</span><span class="o">.</span><span class="n">inference_output_type</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int8</span>
<span class="n">tflite_int8_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>
</pre></div>
</div>
<p>有关全整数量化的更多详细信息，请参考 <a class="reference external" href="https://ai.google.dev/edge/litert/models/post_training_integer_quant#convert_using_integer-only_quantization">tflite官网</a> 。</p>
<p>转换完成后，示例将使用int8 .tflite模型验证测试集上的性能，并生成两个包含了100张测试图像的输入和标签数组的.npy文件，以供稍后在SoC上使用。</p>
<p><code class="file docutils literal notranslate"><span class="pre">convert.py</span></code> 文件中使用了 <a class="reference external" href="https://github.com/onnx/onnx-tensorflow">onnx_tf库</a> 来把onnx格式模型转换成SavedModel格式。也可以使用其他类似功能的转换库：</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PINTO0309/onnx2tf">onnx2tf</a></p></li>
<li><p><a class="reference external" href="https://github.com/google-ai-edge/ai-edge-torch">ai-edge-torch</a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexanderLutsenko/nobuco">nobuco</a></p></li>
<li><p><a class="reference external" href="https://github.com/MPolaris/onnx2tflite">onnx2tflite</a></p></li>
</ul>
</section>
<section id="tflitec">
<h4>步骤 3. 优化Tflite并且转换成C++<a class="headerlink" href="#tflitec" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>使用tflite-micro官方目录提供的 <strong>tflm_model_transforms</strong> 工具，可以通过运行一些TFLM特定的转换来减小文件的大小。它还通过C++ flatbuffer api重新对齐了tflite flatbuffer，这可以在Ameba的某些平台上的加快推理速度。此步骤是可选的，但我们强烈建议执行这步操作：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">tflite</span><span class="o">-</span><span class="n">micro</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">tflite</span><span class="o">-</span><span class="n">micro</span>

<span class="n">bazel</span> <span class="n">build</span> <span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="p">:</span><span class="n">tflm_model_transforms</span>
<span class="n">bazel</span><span class="o">-</span><span class="nb">bin</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">lite</span><span class="o">/</span><span class="n">micro</span><span class="o">/</span><span class="n">tools</span><span class="o">/</span><span class="n">tflm_model_transforms</span> <span class="o">--</span><span class="n">input_model_path</span><span class="o">=&lt;/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">my_model</span><span class="o">.</span><span class="n">tflite</span><span class="o">&gt;</span>

<span class="c1"># output will be located at: /path/to/my_model_tflm_optimized.tflite</span>
</pre></div>
</div>
</li>
<li><p>把.tflite模型和.npy测试数据转换成部署所需的.cc和.h格式文件:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">models</span> <span class="n">int8_tflm_optimized</span><span class="o">.</span><span class="n">tflite</span>
<span class="n">python</span> <span class="n">generate_cc_arrays</span><span class="o">.</span><span class="n">py</span> <span class="n">testdata</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">input_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span> <span class="n">label_int8</span><span class="o">.</span><span class="n">npy</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="id12">
<h4>步骤 4. 在芯片上用Tflite-Micro推理<a class="headerlink" href="#id12" title="Link to this heading"></a></h4>
<p><code class="file docutils literal notranslate"><span class="pre">example_tflm_mnist.cc</span></code> 展示了如何使用训练好的模型在测试数据上运行推理，计算准确率，并且统计内存和延迟情况。</p>
<p>使用 <a class="reference external" href="https://netron.app/">netron</a> 来可视化.tflite文件并查看模型使用的算子。然后实例化操作解析器来注册和使用所需算子。</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">MnistOpResolver</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tflite</span><span class="o">::</span><span class="n">MicroMutableOpResolver</span><span class="o">&lt;</span><span class="mi">4</span><span class="o">&gt;</span><span class="p">;</span>

<span class="n">TfLiteStatus</span><span class="w"> </span><span class="nf">RegisterOps</span><span class="p">(</span><span class="n">MnistOpResolver</span><span class="o">&amp;</span><span class="w"> </span><span class="n">op_resolver</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddFullyConnected</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddConv2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddMaxPool2D</span><span class="p">());</span>
<span class="w">    </span><span class="n">TF_LITE_ENSURE_STATUS</span><span class="p">(</span><span class="n">op_resolver</span><span class="p">.</span><span class="n">AddReshape</span><span class="p">());</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">kTfLiteOk</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>有关使用tflite-micro进行推理的更多详细信息，请参考 <a class="reference external" href="https://ai.google.dev/edge/litert/microcontrollers/get_started#run_inference">tflite-micro official site</a> 。</p>
</section>
<section id="id13">
<h4>步骤 5. 编译<a class="headerlink" href="#id13" title="Link to this heading"></a></h4>
<p>按照 <a class="reference internal" href="#build-tflm-lib"><span class="std std-ref">编译Tensorflow Lite Micro库</span></a> 和 <a class="reference internal" href="#build-tflm-example"><span class="std std-ref">编译示例</span></a> 中的步骤编译固件。</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="0_ai_index_cn.html" class="btn btn-neutral float-left" title="Artificial Intelligence (AI)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../4_ai_voice/0_ai_voice_index_cn.html" class="btn btn-neutral float-right" title="AIVoice" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2025, Realsil。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>

      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>